# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## ReferenceTests Package

ReferenceTests provides visual regression testing infrastructure for Makie. It compares rendered images against reference images to catch visual changes.

## Architecture

### Core Components
- **Image comparison**: Tile-based perceptual difference detection (not pixel-by-pixel)
- **Test recording**: Captures test output images and videos (mp4, gif)
- **Test execution**: Thread-safe test registration and execution
- **HTML reports**: Visual diff viewing (generated by external tools)
- **CI integration**: Automated testing with artifact uploading
- **Cross-backend scoring**: Comparison between different backends (e.g., GLMakie as reference)

### Test Workflow
1. Tests generate images/videos using `@reference_test` macro
2. Compare against downloaded reference images from GitHub releases
3. Record differences using perceptual scoring algorithm
4. Generate score files (scores.tsv, new_files.txt, missing_files.txt)
5. Upload artifacts to CI for review with ReferenceUpdater

## Usage

### Basic Test
```julia
using ReferenceTests
using CairoMakie

@reference_test "my_plot" begin
    # Tests automatically:
    # - Set theme with fixed size (500x500)
    # - Seed stable RNG for reproducibility
    # - Set backend-specific settings (px_per_unit=1, scalefactor=1)
    fig = Figure()
    scatter(fig[1, 1], RNG.randn(100))  # Use RNG module for stable random numbers
    fig
end
```

### Backend-Specific Tests
```julia
# In backend test file
@include_reference_tests GLMakie "refimages.jl"
```

### Test Scores
- `0.0`: Perfect match
- `< 0.05`: Default threshold for GLMakie (see test files)
- `> 0.05`: Likely failure
- `Inf`: Returned when images have different sizes or frame counts differ

## Running Tests

### Local Testing
```julia
# Run reference tests for a backend
using Pkg
Pkg.test("GLMakie")

# Results in: backend/test/reference_images/
# Contains: recorded/, reference/, scores.tsv, new_files.txt, missing_files.txt
```

### Environment Variables
- `REUSE_IMAGES_TAR=1`: Skip downloading reference images if tar already exists
- `ENABLE_COMPUTE_CHECKS=true`: Enable compute graph validation (set in test files)

### CI Testing
- Automatically runs on PRs via GitHub Actions
- Separate jobs per backend (CairoMakie, GLMakie, WGLMakie)
- Uploads results as artifacts (retained for 90 days)
- Posts status check with missing reference count
- Uses Xvfb for headless rendering on Linux

## Updating References

### Using ReferenceUpdater
```julia
using ReferenceUpdater

# From local test run
ReferenceUpdater.serve_update_page_from_dir("GLMakie/test/reference_images/")

# From CI run
ReferenceUpdater.serve_update_page(pr = 1234)
```

### Update Workflow
1. Review visual differences in ReferenceUpdater UI
2. Approve legitimate changes
3. Download and commit updated references
4. Push to branch (references stored in GitHub releases)
5. Re-run tests to verify

### Reference Image Storage
- Stored as GitHub release artifacts with version tags (e.g., `refimages-v0.21`)
- Downloaded automatically during test runs
- Version determined by Makie.jl Project.toml (major.minor only)

## Test Organization

### Directory Structure
```
reference_images/
├── recorded/
│   ├── GLMakie/   # Backend-specific outputs
│   ├── CairoMakie/
│   └── WGLMakie/
├── reference/     # Downloaded from GitHub releases
├── scores.tsv     # Comparison scores for all tests
├── new_files.txt  # Tests without reference images
└── missing_files.txt  # Reference images without tests
```

### Test Categories
- Organized by backend in subdirectories
- Tests can output .png, .jpg, .jpeg, .mp4, or .gif files
- Video comparisons sample up to 10 frames for efficiency
- Special handling for OS-specific rendering differences

## Common Tasks

### Adding New Reference Test
```julia
@reference_test "new_feature" begin
    # Test names must be unique across the test suite
    # Tests are run with memory tracking and timing
    fig = Figure()
    # ... plot code
    fig  # Return Figure, Scene, or VideoStream
end
```

### Supported Return Types
- `Figure` or `Scene`: Saved as PNG
- `VideoStream`: Saved as MP4
- Other FileIO-compatible types: Saved with appropriate extension

### Marking Tests as Broken
```julia
# Mark specific tests or functions as broken
ReferenceTests.mark_broken_tests(
    ["test_name1", "test_name2"],  # Test titles to skip
    functions = [:some_function]     # Functions that make tests broken
```

### Platform-Specific References
Some tests have OS-specific references:
- `test_name_windows.png`
- `test_name_linux.png`
- `test_name_apple.png`

## Debugging Failures

### Local Comparison
```julia
# Compare specific images
using Images, FileIO
ref = load("reference/test.png")
new = load("recorded/test.png")
diff = abs.(ref - new)

# Or use the built-in comparison
score = ReferenceTests.compare_media("recorded/test.png", "reference/test.png")
```

### Score Calculation
- Uses tile-based comparison (approx 30x30 pixel tiles)
- Computes maximum mean difference across all tiles
- Supports RGBA color differences including alpha
- More comparable across different image sizes
- Video scores are maximum score across sampled frames

### Common Failure Causes
1. **Font rendering**: OS-specific
2. **GPU differences**: Floating-point precision
3. **Driver updates**: Rendering changes
4. **Legitimate changes**: New features/fixes

## CI Integration

### GitHub Actions
- Runs on every PR (except docs-only changes)
- Separate jobs per backend with different Julia versions (1.10 and latest)
- Tests run with `continue-on-error` to ensure artifacts upload
- Consolidates results from all backends into single artifact

### Artifact Contents
- `ReferenceImages_{Backend}_{Version}`: Per-backend results
- `ReferenceImages`: Consolidated results with all backends
- `n_missing_refimages`: Count for status check

### CI Workflows
1. **reference_tests.yml**: Main test execution
2. **refimages_status.yaml**: Posts GitHub status check
3. Results trigger ReferenceUpdater bot comments on PRs

## Important Files

### Core Implementation
- **src/ReferenceTests.jl**: Main module and exports
- **src/database.jl**: Test registration, `@reference_test` macro
- **src/runtests.jl**: Test execution and comparison logic
- **src/compare_media.jl**: Image/video comparison algorithms
- **src/image_download.jl**: Reference image management
- **src/stable_rng.jl**: Reproducible random number generation
- **src/cross_backend_scores.jl**: Cross-backend comparison tools

### Test Files
- **tests/*.jl**: Categorized test definitions
- Backend-specific: **{Backend}/test/runtests.jl**
- Custom tests: **{Backend}/test/{backend}_refimages.jl**

## Performance Tips

1. **Batch Tests**: Group related tests in single files
2. **Resolution**: Default 500x500, use minimum needed
3. **Memory**: Tests run GC after each test, monitor with output
4. **Videos**: Limited to 10 frame comparisons for efficiency
5. **Stable RNG**: Use `RNG.rand()` instead of `rand()` for reproducibility

## Troubleshooting

### Missing References
- New tests without references appear in new_files.txt
- Upload via ReferenceUpdater after review
- CI will fail until references are added

### Flaky Tests
- Always use `RNG` module functions for randomness
- Test registration prevents duplicate names
- Mark consistently failing tests as broken

### Large Diffs
- Check scores.tsv for exact difference values
- Inf score means size mismatch or frame count difference
- Cross-backend scores help identify backend-specific issues

### Test Execution
- Tests run in registration order with counter
- Each test reports memory usage and timing
- Failed tests still upload artifacts for debugging

## Dependencies

ReferenceTests includes dependencies for:
- Image handling: Images, FileIO, ImageShow
- Video processing: Makie's built-in video tools
- Stable randomness: StableRNGs
- Test data: Various plotting libraries (GeometryBasics, Colors, etc.)
- CI artifacts: Tar, Downloads

## Recent Updates

### Key Features
- Compute graph integration for validation
- Cross-backend comparison tooling
- Improved video frame sampling
- Tile-based perceptual comparison
- Automatic theme and RNG setup in tests